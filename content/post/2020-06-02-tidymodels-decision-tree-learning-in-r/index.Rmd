---
title: 'Tidymodels: Decision Tree Learning in R'
author: Cianna Bedford-Petersen, Chris Loan & Brendan Cullen
date: '2020-06-02'
slug: []
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-06-02T10:57:23-07:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

# Intro

Tidyverse’s newest release has recently come together to form a cohesive suite of packages for modeling and machine learning. The successor to Max Kuhn’s caret package, tidymodel’s collection of tools allows for a tidy approach to your data from start to finish. We’re going to l, walk through the basics for getting off the ground with tidymodels and demonstrate its application to three different decision tree methods for predicting student test scores. For further information about tidymodels packages and capabilities you can visit https://www.tidymodels.org/.

# Setup

Load both the tidyverse and tidymodel packages into your environment. We’ll also load in the psych package to help us with some descriptives for our data.

```{r message=FALSE}
library(here)
library(tidyverse)
library(tidymodels)
library(skimr)
library(xgboost)
library(baguette)
library(future)
library(vip)
```

# Import the data

We’re going to be using simulated student test data based on 3rd-8th grade math and reading scores from 189,000 students in the state of Oregon. These data have a variety of student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch) to choose from when thinking about what might predict test scores. To highlight the differences in modeling techniques in this tutorial we’re going to include all variables in our analysis. All school IDs in the data are real, so we can use that information to link the data with other sources. Specifically, we’re also going to pull in some free and reduced lunch data from the National Center for Education statistics and some ethnicity data from the Oregon Department of Education. After loading in our three datasets, we’ll join them together to make one cohesive data set and remove any intermediate data sets from our environment. For the purpose of demonstration, we’ll be sampling 1% of the data to keep computer processing time manageable. 

```{r cache=TRUE, message=FALSE, warning=FALSE}
set.seed(100)

# import train data
dat <- read_csv(here("static", "data", "train.csv")) %>% 
  select(-classification) %>%
  sample_frac(.01)

# import fall membership report data
sheets <- readxl::excel_sheets(here("static", "data", "fallmembershipreport_20192020.xlsx"))

ode_schools <- readxl::read_xlsx(here("static", "data", "fallmembershipreport_20192020.xlsx"),
                                 sheet = sheets[4])

# select relevant vars
ethnicities <- ode_schools %>%
  select(attnd_schl_inst_id = `Attending School ID`,
         attnd_dist_inst_id = `Attending District Institution ID`,
         sch_name = `School Name`,
         contains("%")) %>%
  janitor::clean_names()

names(ethnicities) <- gsub("x2019_20_percent", "p", names(ethnicities))

# join ethnicity data with original train data
dat <- left_join(dat, ethnicities)

# import free and reduced lunch data
frl <- rio::import("https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip",
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades
stu_counts <- rio::import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl <- left_join(frl, stu_counts)

# add frl data to train data
dat <- left_join(dat, frl)
```

# Explore the data

We’ll use the `skim()` function from the `{skimr}` package to take a closer look at our (numeric) variables. The histograms show that many of the predictors are highly skewed, but the tree-based models we'll use below are robust to non-normal data. 

```{r}
dat %>% 
  select(score, -contains("id"), -ncessch) %>% 
  select_if(is.numeric) %>% 
  skimr::skim() %>% 
  select(-starts_with("numeric.p"))
```

While most of our predictors are categorical, we can use `{corrplot}` to better visualize the relationships among the numeric variables.

```{r warning=FALSE}
dat %>% 
  select(-contains("id"), -ncessch, -missing, -not_applicable) %>% 
  select_if(is.numeric) %>% 
  select(score, everything()) %>% 
  cor(use = "complete.obs") %>% 
  corrplot::corrplot()
```


# Split data and resample

The first step of our analysis is to split our data into two separate sets, one for training and one for testing. The training set allows us the flexibility to fit a model and adjust or tune the parameters before evaluating the model’s final performance on our test. We can do this efficiently with the initial_split function. This comes from the rsample package, which is included in the tidymodels package that we loaded earlier. By default this will split your data into 75% for the training set and 25%, but you can change this by setting the prop argument (include a prop argument in our example for clarity). We can also specify the argument strata in order to use stratified sampling. Here, we set strata to our score variable to ensure that our resamples incorporate the full range of scores present in our data (is this what strata does?). Then, we’ll extract the training and the testing data sets from our split object and assign them a name. 

Finally, we’ll resample our data using the v-fold command. Though a slightly confusing name, the v-fold command will give us a k-fold cross-validated version of our training data. This will allow us to better estimate the ability of a machine learning model to predict unseen data, this is particularly important if we plan to tune any parameters but also a good choice to prevent overfitting of our model. Though there are many different forms of cross-validation we could apply here, we’ll stick with k-fold as it’s a popular method known for reducing bias in our estimates.


```{r}
# split the data
split <- initial_split(dat)

# extract the training data
train <- training(split)

# resample the data with 10-fold cross-validation
cv <- vfold_cv(train)
```

# Pre-processing

Before we add in our data to the model, we’re going to set up an object that directs our variables into specific roles, this is called a *recipe*. First, you’ll specify a formula for your model, indicating which variable is your outcome and which are your predictors. Using `.` here will indicate that we want to use all variables other than `score` as predictors. Then, we can specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role or manipulates the data. 

A complete list of possible pre-processing steps can be found here: https://recipes.tidymodels.org/articles/Custom_Steps.html


```{r}
rec <- recipe(score ~ ., train) %>% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %>% # convert `tst_dt` variable to a date 
  update_role(contains("id"), ncessch, new_role = "id vars") %>% # declare ID variables
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %>% # remove varaibles with zero variances
  step_novel(all_nominal()) %>% # prepares test data to handle previously unseen factor levels 
  step_unknown(all_nominal()) %>% # categorizes missing categorical data (NA's) as `unknown`
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars"))  %>% # replaces missing numeric observations with the median
  step_dummy(all_nominal(), -has_role("id vars")) # dummy codes categorical variables
```


# Create a model

The last step before bringing in our data is to specify our model. This will call upon functions from the parsnip package which standardizes language for specifying a multitude of statistical models. There are a few core elements that you will need to specify for each model 

The type of model - this indicates what type of model you choose to fit, each of which will be a different function. We’ll be focusing on decision tree methods using bag_tree(), random_forest(), and boost_tree(). A full list of models can be found here https://www.tidymodels.org/find/parsnip/

The engine - set_engine() calls the package to support the model you specified above.

The mode - set_mode() indicates the type of prediction you’d like to use in your model, you’ll choose between regression and classification. Since we are looking to predict student scores, which is a continuous predictor, we’ll be choosing regression. 

The arguments - set_args() allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered. For these parameters, you can either set a particular value or you can use the tune function to search for the optimal value of each parameter. Tuning requires a few extra steps, so we will leave the default arguments for clarity. For more information on tuning check out ???

# Create a workflow

Up to this point we’ve been setting up a lot of individual elements and now it is time to combine them to create a cohesive framework, called a workflow, so we can run our desired models. First, we’ll use the workflow command and then, we’ll pullin the recipe and model we already created. Below are three examples of specifying models and creating a workflow for different decision tree methods.

# Model Examples

## Bagged trees

A bagged tree approach creates multiple subsets of data from the training set which are randomly chosen with replacement. Each subset of data is used to train a given decision tree. In the end, we have an ensemble of different models. The predictions from all the different trees are averaged together giving us a stronger prediction than one tree alone could. 

### Specify model

```{r}
set.seed(100)
mod_bag <- bag_tree() %>%
  set_mode("regression") %>%
  set_engine("rpart", times = 10) # 10 bootstrap resamples
```

### Create workflow

```{r}
wflow_bag <- workflow() %>% 
  add_recipe(rec) %>%
  add_model(mod_bag)
```

### Fit the model

```{r cache=TRUE, message=FALSE}
set.seed(100)
plan(multisession)

fit_bag <- fit_resamples(
  wflow_bag,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))
```

### Visualize

* The plot below shows counts of each tree's root node.

```{r echo=FALSE}
# extract roots
bag_roots <-  function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %>% 
  select(-.extracts) %>% 
  unnest(cols = c(models)) %>% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %>%
  select(root)  
}

# plot
bag_roots(fit_bag) %>% 
  ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + 
  geom_bar() + 
  coord_flip() + 
  labs(x = "root", y = "count")
```


## Random forest

Random forest is similar to bagged tree methodology but goes one step further. In addition to taking random subsets of data, the model also draws a random selection of features. Instead of utilizing all features, the random subset of features allows more predictors to be eligible root nodes. This is particularly useful for handling high dimensionality data.

### Specify the model

```{r}
set.seed(100)
mod_rf <-rand_forest() %>%
  set_engine("ranger",
             num.threads = parallel::detectCores(), #argument from {ranger}
             importance = "permutation", #argument from {ranger}
             verbose = TRUE) %>% #argument from {ranger}
  set_mode("regression") %>% 
  set_args(trees = 1000)
```

### Create workflow

```{r}
wflow_rf <- workflow() %>% 
  add_model(mod_rf) %>% 
  add_recipe(rec)
```

### Fit the model

```{r cache=TRUE, message=FALSE}
set.seed(100)
plan(multisession)

fit_rf <- fit_resamples(
  wflow_rf,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)
```

### Visualize

```{r echo=FALSE}
# extract roots
rf_tree_roots <- function(x){
  map_chr(1:1000, 
           ~ranger::treeInfo(x, tree = .)[1, "splitvarName"])
}

rf_roots <- function(x){
  x %>% 
  select(.extracts) %>% 
  unnest(cols = c(.extracts)) %>% 
  mutate(fit = map(.extracts,
                   ~.x$fit$fit$fit),
         oob_rmse = map_dbl(fit,
                         ~sqrt(.x$prediction.error)),
         roots = map(fit, 
                        ~rf_tree_roots(.))
         ) %>% 
  select(roots) %>% 
  unnest(cols = c(roots))
}

# plot
rf_roots(fit_rf) %>% 
  group_by(roots) %>% 
  count() %>% 
  arrange(desc(n)) %>% 
  filter(n > 75) %>% 
  ggplot(aes(fct_reorder(roots, n), n)) +
           geom_col() + 
           coord_flip() + 
  labs(x = "root", y = "count")
```

## Boosted trees

Boosted trees are a type of additive model that makes predictions by combining decisions from a sequence of base models. These models are learned in succession with the first learners fitting simple models and then analyzing data for errors, with the goal of solving for the net error from the prior tree. 

### Specify the model

```{r}
mod_boost <- boost_tree() %>% 
  set_engine("xgboost", nthreads = parallel::detectCores()) %>% 
  set_mode("regression")
```

### Create workflow

```{r}
wflow_boost <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mod_boost)
```

### Fit the model

```{r cache=TRUE, message=FALSE}
set.seed(100)
plan(multisession)

fit_boost <- fit_resamples(
  wflow_boost, 
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE)
)
```

### Visualize

* Probs don't include graph here, just explain learning rate/what makes boosted trees unique 

# Model evaluation

After running these three models, it’s time to evaluate their performance. We can do this with `tune::collect_metrics()`.

```{r}
collect_metrics(fit_bag) %>% 
  bind_rows(collect_metrics(fit_rf)) %>%
  bind_rows(collect_metrics(fit_boost)) %>% 
  filter(.metric == "rmse") %>% 
  mutate(model = c("bag", "rf", "boost")) %>% 
  select(model, everything()) %>% 
  knitr::kable()
```

* Show metrics in table and rmse/rsq graph

# Apply model to test data

The final step is to apply the model we have chosen to our test data. 

```{r cache=TRUE}
# bagged trees
final_fit_bag <- last_fit(
  wflow_bag,
  split = split
)

# random forest
final_fit_rf <- last_fit(
  wflow_rf,
  split = split
)

# boosted trees
final_fit_boost <- last_fit(
  wflow_boost,
  split = split
)
```


```{r}
# show performance on test data
collect_metrics(final_fit_bag) %>% 
  bind_rows(collect_metrics(final_fit_rf)) %>%
  bind_rows(collect_metrics(final_fit_boost)) %>% 
  filter(.metric == "rmse") %>% 
  mutate(model = c("bag", "rf", "boost")) %>% 
  select(model, everything()) %>% 
  knitr::kable()
```

# Conclusions


